{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graphs inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we are going to work with some text data. In this folder, you should a text file called 'fullpapers.txt'. This file was generated by converting the proceedings of the EDM (Educational Data Mining) conference of 2018. You can find the proceedings here: http://educationaldatamining.org/EDM2017/proc_files/fullpapers.pdf\n",
    "We are going to explore the different terms that are used by authors of the papers in this conference, which will require some data cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is compare the different papers in terms of the vocabulary used. \n",
    "* open the pdf of the proceedings (fullpapers.pdf); \n",
    "* open the txt of the proceedigs (fullpapers.txt)\n",
    "\n",
    "1) we want to split the data into different papers. Brainstorm a few ideas on how to do that:\n",
    "* use 'Proceedings of the 10th International Conference on Educational Data Mining\"\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x0cZone out no more: Mitigating mind wandering during\\ncomputerized reading\\nSidney K. D’Mello, Caitlin Mills, Robert Bixler, & Nigel Bosch\\nUniversity of Notre Dame\\n118 Haggar Hall\\nNotre Dame, IN 46556, U'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) First we are going to read the fullpapers.txt file \n",
    "# and assign its content to a variable called \"data\"\n",
    "# hint: https://stackoverflow.com/questions/3758147/easiest-way-to-read-write-a-files-content-in-python\n",
    "\n",
    "data = None \n",
    "with open('./fullpapers.txt', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3) To facilitate data processing, we want to split this file\n",
    "# into different pages. Create a list called \"pages\" that \n",
    "# stores the text presented on each page of the pdf\n",
    "\n",
    "pages = data.split('Proceedings of the 10th International Conference on Educational Data Mining')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) because we don't want to deal with upper case / lower case issues\n",
    "# we are going to lower case everything:\n",
    "pages = [x.lower() for x in pages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now we would like to join pages if they below to the same paper. Can you think of keywords we could like for to decided if the current page is starting a new paper? Write down two ideas:\n",
    "1. idea\n",
    "2. idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) create a new list called \"papers\", which is going to contain \n",
    "# all the papers we have. Iterate through all the pages and \n",
    "# add a new element to the list when you have a full paper\n",
    "\n",
    "pages = data.split('Proceedings of the 10th International Conference on Educational Data Mining')\n",
    "pages = [x.lower() for x in pages]\n",
    "\n",
    "papers = []\n",
    "current_paper = ''\n",
    "\n",
    "for page in pages:\n",
    "    if 'introduction' in page and 'abstract' in page:\n",
    "        \n",
    "        papers.append(current_paper)\n",
    "        current_paper = ''\n",
    "        current_paper = current_paper + page\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        current_paper = current_paper + page\n",
    "        \n",
    "        \n",
    "\n",
    "# iterate through the pages and add each paper to the list \"papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) print how many files you have in the \"papers\" list:\n",
    "del papers[0]\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----First paper ---\n",
      " \f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "process\n",
      "----Second paper ---\n",
      " \n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "measuring similarity of educational items using data on\n",
      "learners’ performance\n",
      "jiří řihák\n",
      "\n",
      "faculty of informatics\n",
      "masaryk university\n",
      "brno, czech republic\n",
      "\n",
      "thran@mail.muni.cz\n",
      "abstract\n",
      "educational systems typically contain a large pool of items\n",
      "(questions, problems). using data mining techniqu\n"
     ]
    }
   ],
   "source": [
    "# 8) print the content of the first two paper to make sure it worked\n",
    "# (only print the first 300 characters)\n",
    "\n",
    "print(\"----First paper ---\\n\", papers[0][:300])\n",
    "print(\"----Second paper ---\\n\", papers[1][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) create a new folder called papers; this is where we are \n",
    "# going to save each paper into a separate text file\n",
    "# hint: google \"how to create a new folder with python\"\n",
    "\n",
    "import os\n",
    "os.mkdir(\"Papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) save each paper into its unique file in the \"Papers\" folder\n",
    "# we created above\n",
    "# Hint: \"enumerate\" can provide you with the index of the paper in the list\n",
    "# Feel free to use the following filename for the first paper in the list:\n",
    "# ./Papers/paper0.txt on mac and .\\Papers\\paper0.txt on window\n",
    "\n",
    "pages = data.split('Proceedings of the 10th International Conference on Educational Data Mining')\n",
    "pages = [x.lower() for x in pages]\n",
    "\n",
    "papers = []\n",
    "current_paper = ''\n",
    "\n",
    "for page in pages:\n",
    "    if 'introduction' in page and 'abstract' in page:\n",
    "        \n",
    "        papers.append(current_paper)\n",
    "        current_paper = ''\n",
    "        current_paper = current_paper + page\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        current_paper = current_paper + page\n",
    "        \n",
    "del papers[0]\n",
    "\n",
    "for i,paper in enumerate(papers):\n",
    "    output = open(\".\\Papers\\paper\" + str(i) + \".txt\",\"w\", encoding=\"utf-8\") \n",
    "    output.write(papers[i])\n",
    "    output.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking yourself why we need to save the data into text files (instead of just using the list of papers above). One answer is that when we work with large datastsets, it's useful to save snapshots of our data that is \"clean\". This way we don't have to re-run all the code above and we save time. It also allows us to share data between different notebooks for other types of analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper0.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper1.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper10.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper11.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper12.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper13.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper14.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper15.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper16.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper2.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper3.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper4.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper5.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper6.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper7.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper8.txt',\n",
       " 'C:\\\\Users\\\\Jazib Zahir\\\\Documents\\\\GitHub\\\\S435-Week4-CleaningData\\\\week4-data-cleaning-jzahir\\\\Papers\\\\paper9.txt']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11) We are going to practice your \"glob\" skills - find all the \n",
    "# text files in the \"Papers\" folder with a glob command!\n",
    "\n",
    "import glob \n",
    "\n",
    "glob.glob(r'C:\\Users\\Jazib Zahir\\Documents\\GitHub\\S435-Week4-CleaningData\\week4-data-cleaning-jzahir\\Papers\\*.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 12) iterate through each of the text files and read their contents in the variable below:\n",
    "text_list = []\n",
    "import os\n",
    "\n",
    "path = r'C:\\Users\\Jazib Zahir\\Documents\\GitHub\\S435-Week4-CleaningData\\week4-data-cleaning-jzahir\\Papers'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    " f = open(os.path.join(path,filename), encoding=\"utf8\")\n",
    " lines = f.read()\n",
    " text_list.append(lines) \n",
    " \n",
    "print(len(text_list))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>1026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>students</th>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>=</th>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.32</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(.41)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.76</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(.51)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5),</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>establishing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stays</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>display).</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action,</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agreed</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agent,</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one)2.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session).</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utters</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“continue”</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>panel,</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>point.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gains.3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“stay</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“yes”,</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opening</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short).</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monitorrecommend</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delivered,</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15893 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  abs_freq\n",
       "the                   5663\n",
       "of                    3402\n",
       "and                   2704\n",
       "to                    2406\n",
       "a                     2028\n",
       "in                    2026\n",
       "for                   1390\n",
       "that                  1052\n",
       "on                    1049\n",
       "we                    1026\n",
       "is                     997\n",
       "with                   788\n",
       "as                     675\n",
       "learning               638\n",
       "this                   624\n",
       "are                    577\n",
       "data                   512\n",
       "by                     499\n",
       "from                   473\n",
       "be                     435\n",
       "students               421\n",
       "student                407\n",
       "=                      393\n",
       "an                     388\n",
       "or                     362\n",
       "were                   360\n",
       "our                    358\n",
       "each                   350\n",
       "was                    348\n",
       "model                  345\n",
       "...                    ...\n",
       ".32                      1\n",
       "21.4                     1\n",
       "(.41)                    1\n",
       ".76                      1\n",
       "(.51)                    1\n",
       "5),                      1\n",
       "establishing             1\n",
       "stays                    1\n",
       "123                      1\n",
       "15.3                     1\n",
       "18.6                     1\n",
       "display).                1\n",
       "action,                  1\n",
       "agreed                   1\n",
       "agent,                   1\n",
       "one)2.                   1\n",
       "session).                1\n",
       "utters                   1\n",
       "“continue”               1\n",
       "panel,                   1\n",
       "point.                   1\n",
       "gains.3                  1\n",
       "“stay                    1\n",
       "“yes”,                   1\n",
       "122                      1\n",
       "opening                  1\n",
       "short).                  1\n",
       "monitorrecommend         1\n",
       "delivered,               1\n",
       "writing.                 1\n",
       "\n",
       "[15893 rows x 1 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13) Now we are going to compute the frequency of each word across all \n",
    "# documents. Feel free to use the link below to help you!\n",
    "# hint: https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency\n",
    "# (look at the first block of code in the article)\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text in text_list:\n",
    "    for word in text.split():\n",
    "        word_freq[word] += 1\n",
    "\n",
    "pd.DataFrame.from_dict(word_freq, orient='index') \\\n",
    ".sort_values(0, ascending=False) \\\n",
    ".rename(columns={0: 'abs_freq'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     abs_freq\n",
       "the      5663\n",
       "of       3402\n",
       "and      2704\n",
       "to       2406\n",
       "a        2028"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14) If you haven't done so already, create a dataframe from the dictionary\n",
    "# and print the head of the dataframe\n",
    "\n",
    "df = pd.DataFrame.from_dict(word_freq, orient='index') \\\n",
    ".sort_values(0, ascending=False) \\\n",
    ".rename(columns={0: 'abs_freq'})\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a problem with the dataframe above? Is there data meaningful?  The most common words are not significant in terms of meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) We are going to remove the following stop words, so that we see more interesting \n",
    "# keywors. Feel free to use the list and hint below to help you:\n",
    "# hint: https://stackoverflow.com/questions/43716402/remove-row-index-dataframe-pandas\n",
    "STOPWORDS = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "           'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "STOPWORDS1 = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','neither','no','nor',\n",
    "           'not','of','off','often','on','only','or','other','our','own','rather',\n",
    "             'say','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','to','too','us',\n",
    "             'was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "\n",
    "df1 = df.drop(STOPWORDS1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>learning</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>students</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>student</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>=</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>each</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>model</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>more</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>using</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>used</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>performance</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>between</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>number</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>two</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>based</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>set</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>different</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>educational</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>models</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>results</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    level_0        index  abs_freq\n",
       "0         0     learning       638\n",
       "1         1         data       512\n",
       "2         2     students       421\n",
       "3         3      student       407\n",
       "4         4            =       393\n",
       "5         5         each       350\n",
       "6         6        model       345\n",
       "7         7         more       310\n",
       "8         8        using       278\n",
       "9         9         used       258\n",
       "10       10  performance       241\n",
       "11       11      between       231\n",
       "12       12       number       213\n",
       "13       13          two       207\n",
       "14       14        based       198\n",
       "15       15          set       184\n",
       "16       16    different       179\n",
       "17       17  educational       173\n",
       "18       18       models       172\n",
       "19       19      results       171"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16) print the top 20 words of your new dataframe:\n",
    "\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 17 plot the top 20 results above as a histogram: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df1.head(20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell from this historgram? What do EDM researchers seem to care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are improvements you could add to our data cleaning process? Write at least three things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count word frequencies per paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the previous section gave us an overall description of the word frequency for all the papers, it would be interesting to look at each individual paper. This is what we are going to do below, by focusing on the top 30 terms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) save the top 30 words from the dataframe above \n",
    "# in a new variable called \"top_words\"\n",
    "\n",
    "top_words = df1[\"index\"].values\n",
    "\n",
    "top_words = top_words[:30]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>zone out no more: mitigating mind wandering d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>york.</th>\n",
       "      <td>7418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>\\n\\n15\\n\\n\f",
       "measuring similarity of educational...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008.</th>\n",
       "      <td>6491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>\\n\\n87\\n\\n\f",
       "generalizability of face-based mind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "text   \n",
       "zone out no more: mitigating mind wandering d...\n",
       "york.                                               7418\n",
       "text   \\n\\n15\\n\\n\n",
       "measuring similarity of educational...\n",
       "2008.                                               6491\n",
       "text   \\n\\n87\\n\\n\n",
       "generalizability of face-based mind..."
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 19) We are now going to construct a new dataframe where each row is a paper, \n",
    "# each column is one of the top 30 words used and each cell is a count of this word. \n",
    "# NOTE: make sure you add another field called \"text\" where you're going to store the \n",
    "# actual text of the paper. \n",
    "# Hint: build a list of dataframes (one for each papers), \n",
    "# and use the concat function from pandas to concatenate them!\n",
    "d = []\n",
    "\n",
    "for text in text_list:\n",
    "    dic = {}\n",
    "    dic['text'] = text\n",
    "    # iterate through the top words, add counts to the dictionary\n",
    "    # and append the results to the list above (d)\n",
    "    for words in top_words:    \n",
    "        count = 0\n",
    "        for words in text.split():\n",
    "             count += 1\n",
    "        dic[words] = count \n",
    "    df = pd.DataFrame.from_dict(dic, orient='index')\n",
    "    d.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "        \n",
    "\n",
    "# concatenate the list d into a dataframe\n",
    "\n",
    "dataframe = pd.concat(d)\n",
    "\n",
    "dataframe.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'learning'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-313-333b1507c062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# what can you say from it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'learning'"
     ]
    }
   ],
   "source": [
    "# 20) create a scatter plot of the words 'learning' and 'data'\n",
    "# what can you say from it?\n",
    "\n",
    "plt.scatter(dataframe['learning'],dataframe['data'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21) annotate each point with the index number of the dataframe\n",
    "# hint: https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22) what are the two extreme papers, \n",
    "# i.e., papers with more occurences for each term on each axis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23) plot the histogram of the paper that had high counts of \"data\"\n",
    "# hint: https://stackoverflow.com/questions/52392728/create-a-histogram-based-on-one-row-of-a-dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24) plot the histogram of the paper that had high counts of \"learning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25) what can you observe? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26) print the first 1000 characters of each paper. \n",
    "\n",
    "\n",
    "# Is your interpretation confirmed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to work with Regex formulas to extract part of the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27) we are going to work on the first paper to make sure that our \n",
    "# regex works. Just retrieve the text and assign it to a variable below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28) find the text between the words 'abstract' and \n",
    "# 'introduction' for the first paper using a regex\n",
    "# https://stackoverflow.com/questions/12736074/regex-matching-between-two-strings/12736203\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29) find the text between the words 'abstract' and \n",
    "# 'introduction' for the first paper using the .index() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30) add a new column namd \"abstract\" to the dataframe above \n",
    "# and initialize it with an empty string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now add the abstracts to each row of the dataframe using either\n",
    "# of the two methods above\n",
    "# Hint: https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31) print your abstracts (they should contain a lot of \\n = carriage return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32) clean the abstract column using the \"apply\" function with a lambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing documents using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33) now we are going to do something a little more advanced:'\n",
    "# we are going to compute the similarity between two texts\n",
    "# using a method called tf-idf (we'll talk more about it later)\n",
    "# Hint: https://stackoverflow.com/questions/43631533/similarity-between-two-text-documents-in-python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34) What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35) repeat the same procedure with the entire papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36) What are two documents that seem to be very similar?\n",
    "# print their abstract: \n",
    "# print the first 1000 characters of each paper. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37) what seems to be similar between them? \n",
    "\n",
    "# they both talk about analyzing questions and answers from students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free exploration (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- try to extract the names of the author\n",
    "- find a way to get the top words shared across two texts\n",
    "- use a regex (or any other method) to get the list of references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
